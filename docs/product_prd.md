# Dugong

## The Embodied K2 Agent for Physical and Digital Spaces

⸻

1. Executive Summary

Dugong is an embodied K2-powered agent that transforms foundation model reasoning into dynamically generated spatial interfaces.

Unlike traditional AI systems that output text, Dugong converts reasoning, planning, and agent execution into real-time environment synthesis across physical (e.g., HoloBox) and digital (web/mobile) platforms.

Dugong does not merely respond.
It constructs.

Powered by K2 V2, K2 Think V2, and orchestrated through OpenClaw, Dugong externalizes cognition into space — turning environments into programmable output surfaces for intelligence.

⸻

2. Problem Statement

Modern AI interfaces are fundamentally limited:
	•	They rely on text-based interaction
	•	They compress complex reasoning into linear language
	•	They separate agent execution from user perception
	•	They lack embodiment and spatial presence

Even advanced foundation models still output primarily text.

There is no system that:
	•	Converts reasoning into structured spatial representations
	•	Treats environment as a programmable interface
	•	Integrates agent planning, tool execution, and visualization in real time
	•	Embodies intelligence across physical and digital surfaces

This creates a gap between cognition and interaction.

⸻

3. Vision

To introduce a new output modality for foundation models:

From language → to space
From response → to construction
From interface → to environment

Dugong externalizes reasoning as spatial structures and treats the scene itself as a dynamic interface compiled at runtime.

⸻

4. Core Concept

4.1 Spatial Externalization of Cognition

Traditional pipeline:
User → Prompt → Text

Dugong pipeline:
User → Intent → Reasoning Plan → Agent Actions → Scene Synthesis

The environment becomes the response.

⸻

4.2 Dugong as a Runtime Interface Compiler

Dugong functions as a runtime compiler that translates:
	•	Natural language intent
	•	Contextual memory
	•	Agent tool outputs
	•	Multi-step reasoning plans

Into:
	•	Scene graphs
	•	3D objects
	•	Dynamic UI components
	•	Interactive spatial elements

The avatar is the embodiment layer of a deeper agentic system controlling the environment.

⸻

5. System Architecture

5.1 Intelligence Layer

K2 Think V2
	•	Multi-step reasoning
	•	Task decomposition
	•	Structured planning
	•	Visualization intent generation

Outputs:
	•	JSON-based scene plans
	•	Action graphs
	•	Tool call sequences

K2 V2
	•	Conversational synthesis
	•	Contextual explanation
	•	Narrative alignment with spatial outputs

⸻

5.2 Orchestration Layer (OpenClaw)

OpenClaw manages:
	•	Stateful sessions
	•	Tool routing
	•	Plan execution
	•	Scene command dispatch
	•	Agent workflow management

⸻

5.3 Scene Control Layer

Scene Interpreter:
	•	Converts structured plans into scene graphs
	•	Spawns objects at runtime
	•	Renders data-driven UI components
	•	Animates state transitions
	•	Maintains spatial state

The scene is not decorative.

It is the primary output surface.

⸻

6. Dynamic Scene as Interface

Dugong treats the environment as programmable UI.

It can dynamically generate:
	•	Data panels
	•	Charts and graphs
	•	Simulation environments
	•	Workflow diagrams
	•	Structured knowledge graphs
	•	Live tool outputs
	•	System states
	•	Interactive components

Agent actions map directly to spatial transformations.

Agent Operation	Spatial Result
Plan Step	Object Instantiation
Tool Output	Data Panel Render
State Update	Scene Transition
Simulation	Animated Environment
Analysis Result	Visual Breakdown

Speech complements space — it does not replace it.

⸻

7. Why Dugong Is Not a 3D Chatbot

Dugong is fundamentally different from:
	•	Pre-rendered holographic demos
	•	Avatar-based chat overlays
	•	Decorative 3D assistants

It:
	•	Generates scene graphs at runtime
	•	Converts reasoning into structured visual outputs
	•	Executes agent workflows visibly
	•	Treats space as a computational surface

The avatar is not the product.
The spatial reasoning engine is.

⸻

8. Deployment Surfaces

Dugong is hardware-agnostic.

Physical Mode
	•	HoloBox deployment
	•	Campus installation
	•	Event demonstration

Web Mode
	•	Browser-based 3D environment
	•	Interactive visualization

Mobile / AR Mode
	•	Lightweight spatial rendering
	•	Portable embodied interaction

Same intelligence.
Different embodiment layers.

⸻

9. Build Week Objectives

This project demonstrates:

Deep K2 Integration
	•	K2 Think drives structured planning
	•	K2 V2 handles adaptive explanation
	•	Reasoning directly influences spatial output

OpenClaw Orchestration
	•	Tool routing
	•	Plan execution
	•	Stateful agent behavior

Working End-to-End System

Voice → Reasoning → Agent Execution → Spatial Interface

Innovation
	•	New output modality for foundation models
	•	Spatial reasoning visualization
	•	Agent-driven environment synthesis

⸻

10. Example Interaction Flow

User:
“Simulate a reinforcement learning agent.”

K2 Think:
	•	Decomposes simulation
	•	Plans environment components
	•	Defines visualization structure

OpenClaw:
	•	Executes simulation tool
	•	Dispatches scene instructions

Scene:
	•	Grid environment appears
	•	Agent moves dynamically
	•	Reward curve updates live

Dugong:
	•	Explains each stage while interacting with objects

The simulation becomes the response.

⸻

11. Strategic Impact

Dugong establishes a research and product direction for:
	•	Embodied AI
	•	Agentic systems
	•	Spatial computing
	•	Foundation model visualization
	•	Cognitive interface design

It positions MBZUAI at the frontier of agent-based embodied intelligence.

⸻

12. Positioning Statement

Dugong is the Embodied K2 Agent for Physical and Digital Spaces.

It introduces spatial interface generation as a new output modality for foundation models — transforming reasoning into constructed environments.
